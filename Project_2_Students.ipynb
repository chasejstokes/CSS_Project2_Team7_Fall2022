{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fdc81b08"
   },
   "source": [
    "# Computational Social Science Project #2 \n",
    "\n",
    "*Group number:* 7\n",
    "\n",
    "*Group members:* Austin Biehl, Peter Soyster, Chase Stokes\n",
    "\n",
    "*Semester:* Fall 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9df765b3"
   },
   "source": [
    "Below we fill in some of the code you might use to answer some of the questions. Here are some additional resources for when you get stuck:\n",
    "* Code and documentation provided in the course notebooks  \n",
    "* [Markdown cheatsheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet) to help with formatting the Jupyter notebook\n",
    "* Try Googling any errors you get and consult Stack Overflow, etc. Someone has probably had your question before!\n",
    "* Send KQ a pull request on GitHub flagging the syntax that's tripping you up "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1799bf97"
   },
   "source": [
    "## 1. Introduction/Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9e6af7b7"
   },
   "source": [
    "#### a) Import relevant libraries\n",
    "Add the other libraries you need for your code below and/or as you go. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b8b9fc38"
   },
   "outputs": [],
   "source": [
    "# import libraries you might need here \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "# use random seed for consistent results \n",
    "np.random.seed(273)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "58ff05e5"
   },
   "source": [
    "#### b) Read in and inspect data frame \n",
    "Read in the data frame and look at some of its attributes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 415
    },
    "collapsed": true,
    "id": "db04b99a",
    "outputId": "e8eee46b-46a4-4609-d1d6-1ed8c4b04860"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-050f982ebdac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m diabetes = pd.read_csv(\"Diabetes with Population Info by County 2017.csv\", \n\u001b[1;32m      2\u001b[0m                        \u001b[0;31m#CountyFips needs to be a string so leading 0 isn't dropped (this is only if you want to make choropleth map):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                        dtype={\"CountyFIPS\": str}) \n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdiabetes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Diabetes with Population Info by County 2017.csv'"
     ]
    }
   ],
   "source": [
    "diabetes = pd.read_csv(\"Diabetes with Population Info by County 2017.csv\", \n",
    "                       #CountyFips needs to be a string so leading 0 isn't dropped (this is only if you want to make choropleth map): \n",
    "                       dtype={\"CountyFIPS\": str}) \n",
    "diabetes.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4fc069eb"
   },
   "outputs": [],
   "source": [
    "# look at the dimensions of the diabetes data frame\n",
    "print('shape: ', diabetes.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b7a87224",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 100) # tells pandas how many rows to display when printing so results don't get truncated\n",
    "\n",
    "# look at the data types for each column in diabetes df \n",
    "print('data types:', diabetes.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "efd204fe"
   },
   "source": [
    "Immediately, we see that some of the features that should be numeric (e.g., Diabetes_Number, Obesity_Number,  and Physical_Inactivity_Number) are not. We can check to see what the non-numeric values are in a column where we are expecting numeric information with a combination of `str.isnumeric()` and `unique()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b8ed5365"
   },
   "outputs": [],
   "source": [
    "# Return rows where the column \"Diabetes_Number\" is non-numeric and get the unique values of these rows\n",
    "# the \"~\" below in front of diabetes negates the str.isnumeric() so it only takes non-numeric values\n",
    "print(diabetes[~diabetes[\"Diabetes_Number\"].str.isnumeric()][\"Diabetes_Number\"].unique()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "83fb4b99"
   },
   "outputs": [],
   "source": [
    "# Now do the same as above, but for \"Obesity_Number\" :\n",
    "print(diabetes[~diabetes[\"Obesity_Number\"].str.isnumeric()][\"Obesity_Number\"].unique()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7dee5236"
   },
   "source": [
    "The values contained in the two columns above making them objects (rather than integers) appear to be strings like \"No Data\" and \"Suppressed.\" Let's drop those rows in the next section, and also recode Physical_Inactivity_Number to be an integer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ca56a10"
   },
   "source": [
    "#### c. Recode variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7658362c"
   },
   "source": [
    "Convert 'Diabetes_Number', 'Obesity_Number', and 'Physical_Inactivity_Number' to integers below so we can use them in our analysis. Also fill in the object type we want to recode 'sex and age_total population_65 years and over_sex ratio (males per 100 females)' to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "067a2950"
   },
   "outputs": [],
   "source": [
    "# Now do the same as above, but for \"Physical_Inactivity_Number\" :\n",
    "print(diabetes[~diabetes[\"Physical_Inactivity_Number\"].str.isnumeric()][\"Physical_Inactivity_Number\"].unique()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "98b7fbf0"
   },
   "outputs": [],
   "source": [
    "# Diabetes\n",
    "# keep only useful info about our target feature, i.e., where diabetes_number not = 'Suppressed'\n",
    "diabetes = diabetes[diabetes['Diabetes_Number']!=\"Suppressed\"]  # note that the inside reference to the diabetes df identifies the column, and the outer calls specific rows according to a condition \n",
    "\n",
    "# use the astype method on Diabetes_Number to convert it to an integer...if you are not sure, what does the astype() documentation tell you are possible arguments? \n",
    "diabetes['Diabetes_Number'] = diabetes['Diabetes_Number'].astype(\"int64\") \n",
    "\n",
    "# Obesity\n",
    "# keep only useful info about our target feature, i.e., where obesity_number not = 'No Data'\n",
    "diabetes = diabetes[diabetes['Obesity_Number']!=\"No Data\"] \n",
    "\n",
    "# use the astype method on Obesity_Number to convert it to an integer.\n",
    "diabetes['Obesity_Number'] = diabetes['Obesity_Number'].astype(\"int64\") \n",
    "\n",
    "# Physical Inactivity\n",
    "# keep only useful info about our target feature, i.e., where Physical_Inactivity_Number not = 'No Data'\n",
    "diabetes = diabetes[diabetes['Physical_Inactivity_Number']!=\"No Data\"] \n",
    "\n",
    "# use the astype method on Physical_Inactivity_Number to convert it to an integer.\n",
    "diabetes['Physical_Inactivity_Number'] = diabetes['Physical_Inactivity_Number'].astype(\"int64\") \n",
    "\n",
    "# 65+ sex ratio had one \"-\" in it so let's drop that row first\n",
    "diabetes = diabetes[diabetes['sex and age_total population_65 years and over_sex ratio (males per 100 females)']!= \"-\"]\n",
    "# change to numeric (specifically, integer or float?) from string (because originally included the \"-\" )\n",
    "diabetes['sex and age_total population_65 years and over_sex ratio (males per 100 females)'] = diabetes['sex and age_total population_65 years and over_sex ratio (males per 100 females)'].astype(float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "115f7116"
   },
   "source": [
    "We should probably scale our count variables to be proportional to county population. We create the list 'rc_cols' to select all the features we want to rescale, and then use the `.div()` method to avoid typing out every single column we want to recode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4a72818a"
   },
   "outputs": [],
   "source": [
    "# select count variables to rc to percentages; make sure we leave out ratios and our population variable b/c these don't make sense to scale by population\n",
    "rc_cols = [col for col in diabetes.columns if col not in ['County', 'State', 'CountyFIPS', \n",
    "                                                        'sex and age_total population_65 years and over_sex ratio (males per 100 females)', 'sex and age_total population_sex ratio (males per 100 females)', 'sex and age_total population_18 years and over_sex ratio (males per 100 females)',  \n",
    "                                                        'race_total population']]\n",
    "           \n",
    "diabetes[rc_cols] = diabetes[rc_cols].apply(pd.to_numeric, errors='coerce') # recode all selected columns to numeric\n",
    "\n",
    "# divide all columns but those listed above by total population to calculate rates\n",
    "diabetes[rc_cols] = diabetes[rc_cols].div(diabetes['race_total population'], axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3929433d"
   },
   "source": [
    "Let's check our work. Are all rates bounded by 0 and 1 as expected? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b3216393",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "# inspect recoded values\n",
    "diabetes_summary = diabetes.describe().transpose() # note we use the transpose method rather than .T because this object is not a numpy array\n",
    "  \n",
    "# check recoding \n",
    "with pd.option_context('display.max_rows', 100, 'display.max_columns', None): \n",
    "    display(diabetes_summary.iloc[ : ,[0,1,3,7]]) # select which columns in the summary table we want to present"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "45caad44"
   },
   "source": [
    "#### d. Check for duplicate columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9c34a5bf"
   },
   "source": [
    "There are a lot of columns in this data frame. Let's see if there are any are duplicates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b45cd094"
   },
   "outputs": [],
   "source": [
    "# I used Google to figure this out, and adapted this example for our purposes:  \n",
    "# source: https://thispointer.com/how-to-find-drop-duplicate-columns-in-a-dataframe-python-pandas/ \n",
    "def getDuplicateColumns(df):\n",
    "    '''\n",
    "    Get a list of duplicate columns.\n",
    "    It will iterate over all the columns in dataframe and find the columns whose contents are duplicate.\n",
    "    :param df: Dataframe object\n",
    "    :return: List of columns whose contents are duplicates.\n",
    "    '''\n",
    "    duplicateColumnNames = set()\n",
    "    # Iterate over all the columns in dataframe\n",
    "    for x in range(df.shape[1]):\n",
    "        # Select column at xth index.\n",
    "        col = df.iloc[:, x]\n",
    "        # Iterate over all the columns in DataFrame from (x+1)th index till end\n",
    "        for y in range(x + 1, df.shape[1]):\n",
    "            # Select column at yth index.\n",
    "            otherCol = df.iloc[:, y]\n",
    "            # Check if two columns at x 7 y index are equal\n",
    "            if col.equals(otherCol):\n",
    "                duplicateColumnNames.add(df.columns.values[y])\n",
    "    return list(duplicateColumnNames)\n",
    "\n",
    "duplicateColumnNames = list(getDuplicateColumns(diabetes))\n",
    "print('Duplicate Columns are as follows: ')\n",
    "duplicateColumnNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0f9a98ac"
   },
   "outputs": [],
   "source": [
    "# now drop list of duplicate features from our df using the .drop() method\n",
    "diabetes = diabetes.drop(columns=duplicateColumnNames) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7623b7e3"
   },
   "source": [
    "## 2. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e9c5095c"
   },
   "outputs": [],
   "source": [
    "# insert your EDAs and interpretations in this section "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "21c15aa8"
   },
   "outputs": [],
   "source": [
    "plt.scatter(diabetes.Diabetes_Number, diabetes.Obesity_Number, \n",
    "            c=diabetes.Physical_Inactivity_Number, cmap='plasma')\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label(\"Proportion of Inactive Population\")\n",
    "plt.xlabel(\"Rate of Diabetes\")\n",
    "plt.ylabel(\"Proportion of Obese Population\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48ba3799"
   },
   "source": [
    "**Austin**: Diabetes appears to be positively associated with obesity. As the proportion of the obese population of a county increases, so too does the rate of diabetes. Additionally, inactivity appears to also correlate with these variables. From the figure, we can see that in counties with high rates of obesity and diabetes also tend to have high rates of inactivity. Taken together, we should expect that both obesity and inactivity will be important predictors of a county's rate of diabetes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-srSZtjePrIv"
   },
   "source": [
    "**Peter's EDA***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6f2f9d51"
   },
   "outputs": [],
   "source": [
    "39\n",
    "# uncomment to install required packages\n",
    "#!pip install -U plotly\n",
    "#!pip install -U dash\n",
    "#!pip install -U kaleido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_cCxLAFwPukh"
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import json\n",
    "with urlopen('https://raw.githubusercontent.com/plotly/datasets/master/geojson-counties-fips.json') as response:\n",
    "    counties = json.load(response)\n",
    "    \n",
    "# This file contains geometry information for US counties, with corresponding FIPS code as column 'id'\n",
    "counties[\"features\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8_K6ORopPxRX"
   },
   "outputs": [],
   "source": [
    "# import packages for plotting\n",
    "import kaleido\n",
    "import plotly.express as px\n",
    "import plotly.io as pio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O04f3SUJP1iR"
   },
   "outputs": [],
   "source": [
    "fig = px.choropleth(diabetes, geojson= counties, locations='CountyFIPS', color='Physical_Inactivity_Number',\n",
    "                           color_continuous_scale=\"Viridis\",\n",
    "                           range_color=(0, 0.2),\n",
    "                           scope=\"usa\",\n",
    "                           labels={'Physical Inactivity Number Number':'Rate of Physical Inactivity',\n",
    "                                  'CountyFIPS': 'County ID'})\n",
    "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9124ffff"
   },
   "source": [
    "**Peter**: Based on the chloropleth figure, there appears to be a relationship between geography and the physical activity. In general, there are higher rates of physical inactivity in the southeastern states of the country, including Louisiana, Mississippi, Alabama, Georgia, South Carolina, and Florida. The figure suggests that community-wide factors, such as culture and social networks, may have an influence upon the rate of physical inactivity in addition to individual-level factors, such as age and physical inactivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8793b1c2"
   },
   "outputs": [],
   "source": [
    "# chase EDA\n",
    "j_plot = sns.jointplot(x='Diabetes_Number', y='sex and age_total population_male', data = diabetes, kind = \"reg\")\n",
    "sns.regplot(x='Diabetes_Number', y='sex and age_total population_male', data=diabetes)\n",
    "j_plot.set_axis_labels('Rate of Diabetes', 'Proportion of Population Male', fontsize=16)\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3d3a8df2"
   },
   "outputs": [],
   "source": [
    "filter_col = ['sex and age_total population_male',\n",
    "              'sex and age_total population_female',\n",
    "              'sex and age_total population_18 years and over_male',\n",
    "              'sex and age_total population_18 years and over_female',\n",
    "              'sex and age_total population_65 years and over_male',\n",
    "              'sex and age_total population_65 years and over_female']\n",
    "\n",
    "plt.bar(x = filter_col, height = diabetes[filter_col].mean())\n",
    "plt.xticks(filter_col, rotation = 90);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "89ea4f02"
   },
   "source": [
    "**Chase**: Diabetes does not appear to be associated with gender. In particular, most counties appear to have a relatively even gender split, except for a few outliers. The rate of diabetes does not increase nor decrease signficantly with this attribute. The chart above shows a straight line of fit, likely due in part to the even spread of gender between counties. Regardless of the reason, the variables relating to gender will likely not be strong predictors for the rate of diabetes in a given county."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "472303bc"
   },
   "source": [
    "## 3. Prepare to Fit Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06c2b969"
   },
   "source": [
    "### 3.1 Finalize Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "648c3a9d"
   },
   "source": [
    "We've already cleaned up the data, but we can make a few more adjustments before partitioning the data and training models. Let's recode 'State' to be a categorical variable using `pd.get_dummies` and drop 'County' using `.drop()` because 'CountyFIPS' is already a unique identifier for the county. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b88dfa46"
   },
   "outputs": [],
   "source": [
    "# create dummy features out of 'State' , which might be related to diabetes rates \n",
    "diabetes_clean = pd.get_dummies(diabetes, \n",
    "                               columns = ['State'],  \n",
    "                               drop_first = True) # only create 49 dummies by dropping first in category\n",
    "\n",
    "# drop 'County' variable\n",
    "diabetes_clean = diabetes_clean.drop(labels = ['County', 'CountyFIPS'],\n",
    "                               axis = 1) # which axis tells python we want to drop columns rather than index rows?\n",
    "\n",
    "# look at first 10 rows of new data frame \n",
    "diabetes_clean.head(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9beb6ff9"
   },
   "source": [
    "### 3.2/3.3 Partition Data and Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8948fddf"
   },
   "source": [
    "Now, we will partition our data to prepare it for the training process. We will use 60% train—20% validation—20% test in this case. More data in the training set lowers bias, but then increases variance in the validation/test sets. Balancing between bias and variance with choice of these set sizes is important as we want to ensure that there is enough data to train on to get good predictions, but also want to make sure our hold-out sets are representative enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d95db4e3"
   },
   "outputs": [],
   "source": [
    "# drop gender based interaction features since not indicated useful in Chase EDA\n",
    "diabetes_clean = diabetes_clean.drop(filter_col, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e510f119"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set y \n",
    "y = diabetes_clean['Diabetes_Number']\n",
    "\n",
    "# X (everything except diabetes, our target)\n",
    "X_unprocessed = diabetes_clean.drop(labels = ['Diabetes_Number'], axis = 1)\n",
    "X_unprocessed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d326d001"
   },
   "source": [
    "We should also preprocess our data. Using the `preprocessing` module from sklearn, let's scale our features so that they are mean-centered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0cf30426"
   },
   "outputs": [],
   "source": [
    "X_unprocessed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e17e6e42"
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "X = preprocessing.scale(X_unprocessed)\n",
    "X = pd.DataFrame(X, columns = X_unprocessed.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "127cd113"
   },
   "source": [
    "We can also get rid of the 0 variance features using the `VarianceThreshold()` method from `feature_selection`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f48f2756"
   },
   "outputs": [],
   "source": [
    "from sklearn import feature_selection\n",
    "\n",
    "def variance_threshold_selector(data, threshold=0):\n",
    "    selector = feature_selection.VarianceThreshold(threshold)\n",
    "    selector.fit_transform(data)\n",
    "    return data[data.columns[selector.get_support(indices=True)]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ed549e48"
   },
   "outputs": [],
   "source": [
    "X = variance_threshold_selector(X)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c21b7aef"
   },
   "source": [
    "And finally, let's split our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ec9413c5"
   },
   "outputs": [],
   "source": [
    "# split the data\n",
    "# train_test_split returns 4 values: X_train, X_test, y_train, y_test, so how do we create a 60-20-20 train-validate-test split? \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(X_train, y_train, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "abf1d1a0",
    "tags": []
   },
   "source": [
    "## 4. Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fe2fe09d"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso, LassoCV, LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2e0f087e"
   },
   "source": [
    "### 4.1 Model Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eba9c618",
    "tags": []
   },
   "source": [
    "#### Model 1: Linear Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b73f588d"
   },
   "source": [
    "Explain why we picked Model 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6f3372e5"
   },
   "source": [
    "#### Model 2: Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "177dc7f6"
   },
   "source": [
    "Like OLS, ridge regression estimates parameters that minimize the sum of the squared residuals; however, unlike OLS, ridge regression also seeks to minimize the sum of the squared coefficients, as a function of a tuning parameter. The rationale behind ridge regression is that by shrinking the estimates toward zero, the variance of the estimated model is reduced. Moreover, ridge regression is particularly well suited to models consisting of a larger number of predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "21af4a2d"
   },
   "source": [
    "#### Model 3: LASSO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "17f32904"
   },
   "source": [
    "Since there are many features included in the model, even after excluding based on the EDA results, the LASSO model allows us perform feature selection computationally. This will be a great help in determining which factors actually contribute to the rate of diabetes by county."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "44851c66"
   },
   "source": [
    "### 4.2 Train Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "be7215b0"
   },
   "source": [
    "#### Model 1: Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1c872054"
   },
   "outputs": [],
   "source": [
    "# model 1 Austin\n",
    "\n",
    "# create a model\n",
    "lin_reg = LinearRegression()\n",
    "\n",
    "# fit the model\n",
    "lin_model = lin_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "914aaf29",
    "tags": []
   },
   "source": [
    "#### Model 2: Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3796c748"
   },
   "outputs": [],
   "source": [
    "ridge_reg = Ridge() \n",
    "ridge_model = ridge_reg.fit(X_train, y_train)\n",
    "ridge_reg_data = pd.DataFrame([ridge_model.coef_, X.columns]).T\n",
    "ridge_reg_data.columns = ['Coefficient', 'Feature']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UjCgRZvZZm1m"
   },
   "outputs": [],
   "source": [
    "print(ridge_model.coef_)\n",
    "print(ridge_model.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1mYqhHoPZotd"
   },
   "outputs": [],
   "source": [
    "figure = plt.figure()\n",
    "figure.subplots_adjust(wspace = .5, hspace=.5)\n",
    "figure.add_subplot(1, 2, 1)\n",
    "sns.barplot(x=\"Coefficient\", y=\"Feature\", data=ridge_reg_data).set_title(\"Ridge Coefficients\")\n",
    "figure.add_subplot(1, 2, 2)\n",
    "sns.barplot(x=\"Coefficient\", y=\"Feature\", data=lin_reg_data).set_title(\"OLS Coefficients\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 183
    },
    "id": "SLnweeSSZqqf",
    "outputId": "6a10a02a-d05f-4e11-abbf-e25b8bf886b4"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-e01692551c9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mridge_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlin_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ridge_model' is not defined"
     ]
    }
   ],
   "source": [
    "print(sum(ridge_model.coef_**2))\n",
    "print(sum(lin_model.coef_**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HwL97ra3ZsX2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85003a28"
   },
   "source": [
    "#### Model 3: LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5a7b2266"
   },
   "outputs": [],
   "source": [
    "# model 3\n",
    "# LASSO\n",
    "lasso_reg = Lasso(max_iter = 10000)\n",
    "lasso_model = lasso_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6def4c79"
   },
   "source": [
    "## 5. Validate and Refine Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8d80843a"
   },
   "outputs": [],
   "source": [
    "#create function to calculate the root mean squared error\n",
    "def rmse(pred, actual):\n",
    "  return np.sqrt(np.mean((pred-actual)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d82b2f4c"
   },
   "source": [
    "### 5.1 Predict on the Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1b894bc0"
   },
   "source": [
    "#### Model 1: Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "426e27b7"
   },
   "outputs": [],
   "source": [
    "# predict the number of riders\n",
    "lin_pred = lin_model.predict(X_validate)\n",
    "\n",
    "# plot the residuals on a scatter plot\n",
    "plt.scatter(y_validate, lin_pred)\n",
    "plt.title('Linear Model (OLS) Predicted v. Actual')\n",
    "plt.xlabel('actual value')\n",
    "plt.ylabel('predicted value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9a4d253c-e762-4466-89ab-f896a8895e42"
   },
   "outputs": [],
   "source": [
    "#calculate the mean squared error of the linear model\n",
    "rmse(lin_pred, y_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W6wsFjOFGw_L"
   },
   "source": [
    "The linear regression model produces a low mean squared error, indicating that the values we forecast were close to the actual values in the validation set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e5ed2108"
   },
   "source": [
    "#### Model 2: Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fYnRY_fzaVC-"
   },
   "outputs": [],
   "source": [
    "# use the model to make predictions\n",
    "ridge_pred = ridge_model.predict(X_validate)\n",
    "\n",
    "# plot the predictions\n",
    "plt.scatter(y_validate, ridge_pred)\n",
    "plt.title('Ridge Model')\n",
    "plt.xlabel('actual values')\n",
    "plt.ylabel('predicted values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f6c6c8a6"
   },
   "outputs": [],
   "source": [
    "rmse(ridge_pred, y_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UkGHZ6gaaEAo"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4e47113f"
   },
   "source": [
    "#### Model 3: LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0a1d0f6c"
   },
   "outputs": [],
   "source": [
    "# validate model 3\n",
    "# LASSO\n",
    "lasso_pred = lasso_model.predict(X_validate)\n",
    "\n",
    "plt.scatter(lasso_pred, y_validate)\n",
    "plt.title('LASSO Model')\n",
    "plt.xlabel('predicted values')\n",
    "plt.ylabel('actual values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "27e9e064"
   },
   "outputs": [],
   "source": [
    "rmse(lasso_pred, y_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8c33eb4e"
   },
   "source": [
    "The LASSO model shows a clear error in its predictions, despite a low RMSE of 0.027."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "93616aa5"
   },
   "source": [
    "### 5.2 Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7185792e"
   },
   "source": [
    "#### Model 1: Linear Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "923fc06c"
   },
   "outputs": [],
   "source": [
    "cutoff = 0.2\n",
    "X_featureselection = X.loc[:, abs(lin_model.coef_) > cutoff]\n",
    "\n",
    "coefs = lin_model.coef_[abs(lin_model.coef_) > cutoff]\n",
    "lin_reg_data = pd.DataFrame([coefs, X_featureselection.columns]).T\n",
    "lin_reg_data.columns = ['Coefficient', 'Feature']\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(50,50))\n",
    "ax = sns.barplot('Coefficient', 'Feature', data = lin_reg_data)\n",
    "ax.set_title(\"OLS Coefficients\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hOZmgeviGgds"
   },
   "outputs": [],
   "source": [
    "# update sets to select these features (from the linear regression)\n",
    "X_train_linearselected = X_train.loc[:, lin_model.coef_ > .2]\n",
    "X_validate_linearselected = X_validate.loc[:, lin_model.coef_ > .2]\n",
    "X_test_linearselected = X_test.loc[:, lin_model.coef_ > .2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e23774b7"
   },
   "source": [
    "#### Model 2: Ridge?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "48625182"
   },
   "outputs": [],
   "source": [
    "param_grid = {'alpha': np.arange(.1, 1, .1),\n",
    "              'fit_intercept': ['True', 'False'],\n",
    "              'solver': ['auto', 'svd', 'cholesky', 'lsqr']}\n",
    "\n",
    "ridge_grid_reg = GridSearchCV(ridge_reg, param_grid, cv=10)\n",
    "ridge_grid_reg.fit(X_train, y_train)\n",
    "\n",
    "best_index = np.argmax(ridge_grid_reg.cv_results_[\"mean_test_score\"])\n",
    "best_ridge_pred = ridge_grid_reg.best_estimator_.predict(X_validate)\n",
    "\n",
    "print('Best CV R^2:', max(ridge_grid_reg.cv_results_[\"mean_test_score\"]))\n",
    "print('Validation R^2:', ridge_grid_reg.score(X_validate, y_validate))\n",
    "print('Validation RMSE', rmse(best_ridge_pred, y_validate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b1d8bf97",
    "tags": []
   },
   "source": [
    "#### Model 3: LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "da329382"
   },
   "outputs": [],
   "source": [
    "np.unique(lasso_model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a382dd86"
   },
   "source": [
    "As shown above, the LASSO model selected has reduced all the coefficients to 0. This means that the predicted value is the same every time. So, we must have selected a lambda value which is too high (i.e., the model default of `alpha = 1.0` is too high). To fix this, we bring in the `LassoCV()` function to select better values of alpha through cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4096b9a5"
   },
   "outputs": [],
   "source": [
    "# model 3 CV\n",
    "# LASSOCV\n",
    "lasso_reg = LassoCV(max_iter = 10000)\n",
    "lasso_model = lasso_reg.fit(X_train, y_train)\n",
    "lasso_pred = lasso_model.predict(X_validate)\n",
    "\n",
    "plt.scatter(lasso_pred, y_validate)\n",
    "plt.title('LASSO Model')\n",
    "plt.xlabel('predicted values')\n",
    "plt.ylabel('actual values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3cbd6224-3fc2-4ba6-a7cc-f11d1ef30fa8"
   },
   "outputs": [],
   "source": [
    "cutoff = 0.001\n",
    "X_featureselection = X.loc[:, abs(lasso_model.coef_) > cutoff]\n",
    "\n",
    "coefs = lasso_model.coef_[lasso_model.coef_ != 0]\n",
    "lasso_reg_data = pd.DataFrame([coefs, X_featureselection.columns]).T\n",
    "lasso_reg_data.columns = ['Coefficient', 'Feature']\n",
    "\n",
    "figure = plt.figure()\n",
    "figure.subplots_adjust(wspace = .5, hspace=.5)\n",
    "figure.add_subplot(1, 2, 1)\n",
    "# Plot for LASSO coefficients here\n",
    "sns.barplot(x = 'Coefficient', y = 'Feature', data = lasso_reg_data)\n",
    "plt.title('LASSO coefficients');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6c5249ef"
   },
   "outputs": [],
   "source": [
    "lasso_model.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5c8d55c1"
   },
   "outputs": [],
   "source": [
    "rmse(lasso_pred, y_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "601fe50d"
   },
   "source": [
    "This RMSE, of 0.018 is lower than the original model with `alpha = 1`, which had an RMSE of 0.027."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e6ed301b"
   },
   "source": [
    "Through cross-validation on the validation data sets, we were able to find the best alpha value, around 0.0004. This is far from the default value of 1 and likely indicates that the LASSO model adds little to a regular `LinearRegression()`. This now serves to help us with relevant feature selection, as LASSO performs feature selection as part of the modeling process. Rather than have no features to choose from, the adjustment to alpha allows us to determine key features which contribute to the predictive ability of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "24d54038"
   },
   "outputs": [],
   "source": [
    "# update sets to select these features (from the lasso regression)\n",
    "X_train_lassoselected = X_train.loc[:, abs(lasso_model.coef_) > cutoff]\n",
    "X_validate_lassoselected = X_validate.loc[:, abs(lasso_model.coef_) > cutoff]\n",
    "X_test_lassoselected = X_test.loc[:, abs(lasso_model.coef_) > cutoff]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5373a3e0"
   },
   "source": [
    "### 5.3 Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1c43865f"
   },
   "source": [
    "#### Chosen Model: ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4f45a5da"
   },
   "outputs": [],
   "source": [
    "# test set model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b7b7772e"
   },
   "source": [
    "### 5.4 Implement a Cross-Validation Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "757a4a4c"
   },
   "source": [
    "#### Chosen Model: ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1be6b541"
   },
   "outputs": [],
   "source": [
    "# cv model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f80baffc"
   },
   "outputs": [],
   "source": [
    "# cv model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1b6fd48f"
   },
   "source": [
    "## 6. Discussion Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "79052ab4"
   },
   "outputs": [],
   "source": [
    "# insert responses for discussion Qs here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86e4d913"
   },
   "source": [
    "### 6.1\n",
    "\n",
    "What is bias-variance tradeoff? Why is it relevant to machine learning problems like this one?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20899596"
   },
   "source": [
    "*Answer*: The bias-variance tradeoff refers to the fact that there is a tradeoff regarding a model's ability to minimize bias and variance. Bias relates to difference between the model we make and the actual, correct value that we are trying to predict. A model with high bias might be oversimplified, and thus have a high amount of errors on both training and test data. Variance refers to the variability of our models predictions for a given data point. Models with a high amount of variance try to really close train on the training data, and therefore often don't perform well on the test data because they are too specific to one particular condition. \n",
    "\n",
    "The bias-variance trade off becomes relevant when trying to create a model given a large amount of parameters, as we were with the diabetes data set. If our model is too simple, and we only put in two or three of the biggest parameters (obesity, physical activity, etc), the model might be clunky and non-specific, missing so much nuance that it is no longer useful. In contrast, if we used ALL the parameters in the data set, parameters that just happened to have small effects in this dataset, but that aren't more broadly useful, might result in us creating a model that is TOO specific, and that wouldn't help us make accurate predictions with new data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9eb37a6"
   },
   "source": [
    "### 6.2\n",
    "\n",
    "Define overfitting, and why it matters for machine learning. How can we address it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9a41fb03"
   },
   "source": [
    "*Answer*: Generally speaking, overfitting is when a statistical model is tuned to the characteristics of a specific dataset in a way that decreases the generalizability of the model to new data. Overfitting is particularly common when a model has a large number of independent variables relative to the number of total observations. As discussed above, sometimes by increasing a models bias (e.g., increasing the extert to which is doesn't try to perfectly predict each data point) you decrese the risk of overfitting and actually end up with more a more generalizable model.  \n",
    "\n",
    "Overfitting is a particularly important issue in machine learning models as these types of models tend to have a large number of independent variables. To address overfitting in machine learning models, researchers often partician the data into three seperate bins \"training data\", \"validation data\", and \"testing data\". By fitting the model to the training data, and testing the generalizability of model performance on the validation data, researchers have the ability to tune the model in a way that maximized generalizability. Once the iterative model building process is complete, the final model is tested with the testing data, to determine the generalized performance of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a793f6ba"
   },
   "source": [
    "### 6.3\n",
    "\n",
    "Discuss your Analysis in 2-3 Paragraphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X7d97KrS_R7Q"
   },
   "source": [
    "*Answer*: In this project, we used several basic machine learning models to predict diabetes rates in US counties, with the goal of developing mechanisms to provide more targeted interventions. There were several stages involved with this process, including exploring the data, creating and training the models, and then testing and selecting a final model for use. \n",
    "\n",
    "To begin with, each of us explored the data in order to get a sense of which variables might be most useful in the construction of a machine learning model. This was a helpful step, as the diabetes data set contained a lot of different factors that could potentially contribute to a final model. Exploratory data analysis gave our team a sense of which factors would likely be central to a final model. In this case, we created scatterplots, bar charts, and choropleths, to get a sense of the data. All our visualizations told the story that counties with higher rates of obesity, and lower rates of physical activity, tended to have higher rates of diabetes. Additionally, race seemed to be important as well, with higher diabetes rates observed in non-white populations. \n",
    "\n",
    "After splitting our data into training, validation, and test data, we created 3 different machine learning models:  a linear regression model, a ridge regression model, and the LASSO model. After training and validating the models, we removed variables from the coefficients from the regression models that were either 0 or near 0, in order to get rid of excess variables that were not contributing to our ability to make accurate predictions. Additionally, we used the LASSO approach to reduce the number of coefficients to 10. Interestingly, all 3 approaches resulted in very similar RMSE values. However, we ultimately decided to use the LASSO model for our final test. We thought that, while it provided only a nominal improvement in prediction accuracy, it was by far the more parsimonious model, and as such might contribute to greater interpretability by policy makers. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "45caad44",
    "2e0f087e",
    "eba9c618",
    "6f3372e5",
    "21af4a2d"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
